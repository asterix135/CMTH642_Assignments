---
title: 'Assignment 3: CMTH642'
author: "Christopher Graham"
date: "November 27, 2015"
output: pdf_document
---

# Overview

This assignment develops a model to predict a subjective quality rating on 
Portuguese red vinho verde wines.  The results we generate are compared with
results obtained by the original paper to work with the data.

We instigated two main changes over the original paper's approach: applying an 
ensemble learning model to the data, and using an undersampling/SMOTE
oversampling algorithm to deal with class imbalance in the data.  

In the end, however, we were not able to improve on the original paper's
overall accuracy, and were only able to marginally improve the quality
with which we predicted minority class membership.

# Import and Split Data

The data is provided at
https://archive.ics.uci.edu/ml/datasets/Wine+Quality and was originally used for
the paper "Modeling wine preferences by data mining from physicochemical
properties" available at
http://repositorium.sdum.uminho.pt/bitstream/1822/10029/1/wine5.pdf

The data set provides provides 11 chemical measurements of Portuguese red wines,
and one quality score assigned by human testers.  The data is complete and
clean.  Pre-processing is described in the original paper.

```{r load, message = FALSE}
# load all libraries required in analysis
require(caret)
require(corrplot)
require(FNN)

# load functions that are used later in analysis
source('ensemble_eval.R')
source('over_under_wrapper.R')
source('find_minor_class_f_stat.R')

red <- read.csv('winequality-red.csv', sep = ';')
# Treating as classification problem so convert quality to factor variable
red$quality <- as.factor(red$quality)

# Divide off a testing set for final validation
# All work will be done only based on the training set
set.seed(25678)
# Note that createDataPartition attempts to balance classes
train_idx <- createDataPartition(y=red$quality, p=0.8, list = FALSE)
training <- red[train_idx,]
testing <- red[-train_idx,]
```

# Check Data Characteristics

## Completeness

```{r check data}
if (sum(complete.cases(testing)) == nrow(testing)) {
    print('All complete cases')
}
```

The data is complete in all variables, and as such there is no need to impute
values.  

## 2-D Visualization

Applying Principal Component Analysis to the data set, we can capture over 99%
of the variance with 2 Principal Components, giving us a pretty good
visualization of the data:

```{r pca visualization, warning=FALSE, message=FALSE}
pc_comp <- prcomp(log10(training[,-12]+1))
plot(pc_comp$x[,1], pc_comp$x[,2],
     col = training$quality,
     main = 'Red Wine - PCA Plot (Log10 Scale)',
     xlab = 'PCA1',
     ylab = 'PCA2')
```

Essentially, it's a mess, with no readily apparent trends.  There seems to be a
bit of a division between some of the major classes, but nothing particularly
clear.  We'll see if we can make sense of it with some machine learning.

## Class Balance

There is a significant concern with the distribution of quality scores in the
data set.  Specifically, over 80% of the wines have 
average ratings (5 or 6), and very few wines get ratings at the more extreme
ends of the rating spectrum.  (3 and 8 are the extreme values awarded, even
though wines were rated on a 10-point scale).  Class imbalance has been
shown to be a significant problem in building an effective Machine Learning
model.  We  provide a strategy for dealing with this below, but first will
look at some other characteristics of the data.

```{r qual histogram}
ggplot(training, aes(x=quality)) +
    geom_histogram(color='black', fill='white') +
    ggtitle('Distribution of Quality Scores') +
    xlab('Quality Score')
```

## Variable Relationships

```{r}
# Check for variables with near zero variability
nearZeroVar(training, saveMetrics = TRUE)

# look at correlation between predictor variables
corrplot(cor(training[-12]), method = 'ellipse', type = 'lower',
         title = 'Correlation of predictor variables', diag=FALSE,
         mar=c(0,0,1,0))

# Compare relationsip of quality to predictor variables
# first normalize the variables so we can compare in one graph
norm_obj <- preProcess(training[,-12], method=c('center', 'scale'))
train_norm <- predict(norm_obj, training)
featurePlot(x=train_norm[,-12], y=train_norm$quality, plot='box',
            main = 'Relation of quality to predictor variables')
```

## Summary of Exploratory Data Analysis

What this preliminary exploration tells us is:

- None of the variables show near zero variability, meaning there is no 
*prima facie* reason to exclude any at the start of our analysis

- For the most part, there does not seem to be a huge amount of collinearity
between variables.  There are some instances of collinearity, where it might be
expected (citric.acid, fixed.acidity and pH), (free.sulfur.dioxide and
total.sulfur.dioxide).  So there may be some room to reduce dimensionality.
However, we will not be doing that in this particular analysis.

- There are some clear relationships between some of the predictor variables 
and wine quality.  The variables volatile.acidity, alcohol and density seem
particularly important.

# Strategies for dealing with Class Imbalance in Provided Data

Since one of the goals for a prediction system would be to help vintners 
identify wines that are likely to be either very good or very bad, the lack
of wine specimens with extreme scores could be problematic for our model
development.

Dealing with class imbalance is a common problem in machine learning.  The 
standard approach in this situation is a combination of under-sampling the 
majority class and over-sampling the minority class to create a distribution
that will help to develop an adequate predictive model.  However, there is a 
persistent concern about the extent to which this under & oversampling should 
occur, and the specific techniques to implement.

While there are different pre-made solutions to this problem, many of them 
are designed around a binary response variable.  Specifically, the R package 
'unbalanced' provides a number of pre-made approaches to dealing with an 
unbalanced data set.  But, unfortunately, it requires a binary response 
variable.

In this case, we are approaching this as a multiple-level classification, 
relying on the classes provided in the original data (reasons for this are 
discussed below). 

I have identified two different approaches to effectively determining an over-/
under-sampling approach for multi-class response variables.  The first is the 
wrapper approach proposed by Chawla et al^[Chawla et al "Automatically 
countering imbalance..."], and the resampling ensemble algorithm proposed by 
Qian et al^[Qian et al. "A resampling ensemble algorithm..."].

Both of these approaches rely on a combination of SMOTE oversampling and random
undersampling.  However, the Chawla paper offers a more coherent (and KDD-Cup
proven!) approach to determining sample levels, and so we will use that
approach here.

Note, however that the algorithms used in Chawla et al require us to identify
a classifier algorithm that will be used to determine stop points for sampling.

So, we need to turn briefly to model selection.

# Model Selection

## Regression vs. Classification

As noted on the source page for this data set, the data lends itself to either
a regression or classification approach.  The original paper to use this data
set adopted a regression approach to the data, arguing that regression allows us
to better evaluate "near miss" predictions (e.g. if the true value is 3, we can
up-score the model if it predicts 4, rather than 7).  The paper used both Neural
Network (NN) and Support Vector Machine (SVM) models, and obtained an accuracy
rate of 64% with a 0.5 error tolerance in quality prediction, and 89% accuracy 
with a 1.0 error tolerance.^[Cortez P, Cerdeira A, Almeida F, Matos T, Reis J.
Modeling wine preferences by data mining from physicochemical properties. 
Decision Support Systems. 2009;47(4):547-53.]

Furthermore, given the somewhat arbitrary way in which quality scores were
derived (the median value from 3 or more judges), it's safe to say that quality
scores are far from reliable values, but rather near approximations.  As such,
regression probably makes most sense.

But Assignment 1 was based around regression, and I'd like to make this one a
bit more challenging, so I'm going to frame this assignment as a classification
problem.  This means the fairest comparison for our results is with the 0.5 
tolerance level in the original paper -- a difference of less than 0.5 would 
round to the correct value.

## Performance Criteria

In order to ensure easy comparability with the original paper, and for 
simplicity of understanding, our primary judge of performance will be overall
classification accuracy.

However, overall accuracy does not indicate how effective our model is at 
identifying minority class examples.  

Note that because we are implementing the Chawla et al algorithm, our over/under 
sampling function will evaluate performance based on the model's f-statistic 
(also known as F1 score).  The closer this is to 1, the better the performance:

\[
    f-measure = \frac{2 * precision * recall}{recall + precision}
\]

The f-statistic is a good overall performance measure, balancing precision and
recall.  We will therefore look at the f-stat for non-average-rated wines.  This 
will give us a better understanding of how well our model identifies bad or 
excellent wines **only**.  For the purposes of this analysis, we define 
non-average-rated wines as having a quality score of 3, 4, 7 or 8.

In order to minimize the impact of the relative overweight in class 7, we will
be using the mean of the f-stats for these four classes, rather than 
calculating an overall f-stat for all non-average-rated classes combined.

The code for the function we will have written to calculate this is:

```{r show f stat code, echo=FALSE}
find_minor_f
```

## Model Selection

Given the relatively weak predictive accuracy (at T=0.5) for the models in the
first paper, it appears that a single model may not be all that great at
classification.  In these instances, one of the more interesting approaches
is *ensemble learning* - essentially combining predictions from multiple models
into one super-model.^[Numerous articles have shown this, but see specifically:
Rokach L. Ensemble-based classifiers. Artificial Intelligence Review.
2010;33(1):1-39.]

In terms of specific ensemble learning implementation, we are going to use the
approach suggested by Jeff Leek of John's Hopkins University in the Coursera 
online course "Practical Machine Learning"^[Course notes available at:
http://sux13.github.io/DataScienceSpCourseNotes/8_PREDMACHLEARN/Practical_Machine_Learning_Course_Notes.pdf, starting on p. 67].

In order to build an ensemble model, we first need to build a number of
individual models to combine into the final approach.  In an attempt to bring
together all we've done in the course (plus a little bit more!), we're going
to use:^[Although we didn't go over all of these in class, I am confident that
I have a good understanding of them, and would be happy to discuss in further
detail if you have questions as to that end.]

- multinomial logistic regression (multinom or mn)
- Naïve Bayes (nb)
- Random Forest (rf)
- Boosting; specifically boosting with trees (gbm)
- Linear Discriminant Analysis (lda) - in case` there are any interesting
differences with Naive Bayes if we don't assume variable independence

We are specifically excluding the two models of the original paper (NN and SVM)
to see if we can do better without the models used there.

Also,  because the SMOTE oversampling algorithm relies on a knn methodology,
we will not be including knn in the base models in order to avoid potentially
introducing erroneous re-classification.

Finally, note that for the final model, the model we will use is a simple
plurality vote among the model responses (the modal value).

The code for the specific model being run is in Appendix A.

## k-fold Cross-validation

Note that our model evaluation is set using 10 repetitions of 10-fold 
cross-validation.  This is set through the trainControl function in the R's 
caret library.

## Normalizing Data

As per the original paper, we will be evaluating the feed-in models based on
standardized data.  This is done through the preProcess function in R's caret
library.

## Dimensionality Reduction

As noted earlier, we can use PCA to simplify the data set to 2 principal
components and with those, capture 99% of the variance in the original data set.
I have not actually implemented this, although it would likely simplify and
speed up some of the computation that follows.  It is worth considering, 
especially if we attempt to apply this to the white wine data set which is 
much larger.

# Implementation of Ensemble Model

## Model baseline testing

As a baseline, before dealing with the class imbalance, let's see how our
model performs:

```{r inital results, cache=TRUE, message=FALSE, warning=FALSE}
# First, get the predictions for the training set
base_results <- ensemble_eval(training)
```

```{r cm base combo}
# confusion matrix for predictions on training data
cm <- confusionMatrix(base_results[[1]], training$quality)
cm$table
cm$overall
```

The model gives us a pretty good result overall, with a predicted 71% accuracy 
on the training data, better than the 64% accuracy (at a 0.5 threshold) from 
the original paper.  

The f-stats for the minority classes (3,4,7 and 8), however, are noticeably
better for our base model than for the original paper:^[Note that because the
original paper predicted 0 true positives for class 3 and class 8, the f-stat
for each of those is technically NaN.  I have substituted with zero to compare.]

```{r base f stat}
# Average non-average-quality f-stat for our model
f_stat <- find_minor_f(cm)

# Average non-average-quality f-stat for original paper
paper_f <- mean(c(0, 0, (2*(1/53)*(1/5))/(1/53+1/5), 
                  (2*(82/199)*(82/140))/(82/199+82/140)))

print(paste('base model f-stat:', f_stat))
print(paste('original paper f-stat', paper_f))
```

```{r comparison data frame, echo=FALSE}
# Store results for various models in a data frame for easy comparison
model_summary <- data.frame(model = 'paper', 
                            accuracy = 0.64, 
                            f_stat = paper_f,
                            stringsAsFactors = FALSE)

```

## Baseline against test set

As a test, we'll apply this model to the test data and see if our results are
legitimate or due to overfitting.

```{r base test preds, warning=FALSE, message=FALSE, cache=TRUE}
base_test <- ensemble_eval(training, testing)
```

```{r test results, warning=FALSE}
cm2 <- confusionMatrix(base_test$final, testing$quality)
cm2$table
cm2$overall

# Average non-average-quality f-stat for our model
test_f <- find_minor_f(cm2)
test_f
```

```{r result update 1, echo=FALSE}
# Update results table
model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[2,1] <-'base_ensemble'
model_summary[2,2] <- cm2$overall[1]
model_summary[2,3] <- test_f
```

When we apply our model to the testing set, we see a relatively significant
degradation in performance - essentially to the level of the original paper,
likely indicating some overfitting in our model.

Importantly, our model is doing a very poor job at predicting minority classes
in the test set -- especially quality ratings of 3, 4, and 8, which have the
weakest representation in our data.

So, in an effort to improve on this, we need to turn to our function for 
undersampling and SMOTE oversampling.

## Ensemble model vs. component models

It's also worth looking at how well our ensemble model is working in 
comparison to each of the individual component parts.  As noted earlier, in
theory, we should be getting better results.

```{r base test components}
cm_mn <- confusionMatrix(base_test$mn, testing$quality)
cm_nb <- confusionMatrix(base_test$nb, testing$quality)
cm_rf <- confusionMatrix(base_test$rf, testing$quality)
cm_gbf <- confusionMatrix(base_test$gbm, testing$quality)
cm_lda <- confusionMatrix(base_test$lda, testing$quality)

# Multinomial
cm_mn$overall[1]; find_minor_f(cm_mn)

# Naive Bayes
cm_nb$overall[1]; find_minor_f(cm_nb)

# Random Forest
cm_rf$overall[1]; find_minor_f(cm_rf)

# Boosting
cm_gbf$overall[1]; find_minor_f(cm_gbf)

# Linear Discriminant Analysis
cm_lda$overall[1]; find_minor_f(cm_lda)
```

This shows some interesting results.  Namely that the random forest model seems
head and tails above the other ones - including our ensemble model - in both
overall prediction accuracy, and in the f-score it returns for the minority 
classes.  The f-score itself isn't eye-poppingly better than either Naive Bayes
or Boosting, but the combination of overall accuracy and f-score makes this
modelling approach stand out as perhaps the best way of analyzing this data.

Also, the ensemble approach does not appear to be working that well in 
comparison to its constituent models.  The ensemble's overall accuracy is about
the mean of the constituent models, and it's minority class f-score is actually
lower than the average of its constituents.

At this point, it would probably be most logical to drop multinomial 
regression and Linear Discriminant Analysis from our approach, as neither
shows excellence on either performance metric we have adopted.  But in a spirit
of inquiry, let's forge ahead and see what happens to all the models once
we reshape the data.

# Undersampling/SMOTE oversampling implementation

As discussed earlier, I have decided to implement the "Wrapper Undersample
SMOTE Algorithm" developed by Chawla et al as a way to re-balance the provided
data.

Simplified, the algorithm works as follows:

1) Establish a baseline performance level for each majority and minority class

2) For each majority class, undersample by 10% until either minority class
predictions stop improving, or until majority class predictions degrade by 5%.
There is no guarantee that the majority class will in fact be undersampled.

3) For each minority class, SMOTE oversample by 100%, and continue this 
until that minority class performance is increased by less than 5% three times.
This guarantees at least a 800% oversample rate for each class.^[The first 
oversample is based on the original data, giving us a class size double the 
original.  The next oversample is based on the oversampled *and* original data, 
giving us results that are 4x the original data, etc.]

The specific R code for my implementation is shown in Appendix 2.  The
algorithm returns a reshaped data set that will be used to generate our
final predictive model.^[Because I don't have a great grasp of how R memory
allocation works, I was unable to run this through knitr.  My laptop
ran out of memory in executing the algorithm.  Instead I ran the code through 
R's console with occasional memory purges, saved the final data set to disk and
loaded it into the RMD file for further manipulation.]

```{r reshape, eval=FALSE}
# algorithm needs a named vector of class categories
#       with -1 = minority; 1 = majority; 0 = neutral
class_list <- c(-1,-1,1,1,0,-1)
names(class_list) <- c(3,4,5,6,7,8)
new_training <- wrapper(class_list = class_list, train_set = training,
                        eval_fun = ensemble_eval, seed=TRUE)
```

```{r load reshaped, echo=FALSE}
new_training <- read.csv('reshaped.csv', row.names = 1)
new_training$quality <- as.factor(new_training$quality)
```

First, let's see how our new distribution compares to the original training
data:

```{r examine new, echo=FALSE}
barplot(rbind(table(training$quality), 
              table(new_training$quality)), 
        main = 'Comparison of reshaped & original data',
        xlab = 'Quality Rating',
        col = c('darkblue', 'red'),
        beside=TRUE)
legend('topright', c('original', 'reshaped'), 
       col = c('darkblue', 'red'), 
       lwd=10)
box()
```

As you can see, the majority classes decreased by a small amount, while the
majority classes have seen a significant increase in representation.  Class 4
now approximates the majority classes.  Class 3 and 8 are still relatively 
underrepresented.

But more importantly, let's see if our reshaped data gives us a better model.
We will look at how the model works for our testing data set.

```{r new model testing, cache=TRUE, warning=FALSE, message=FALSE}
new_test_results <- ensemble_eval(new_training, testing)
```

```{r reshaped comparison, message=FALSE, warning=FALSE, echo=FALSE}
# confusion matrix for model applied to testing data
cm3 <- confusionMatrix(new_test_results$final, testing$quality)
cm3$table
cm3$overall

model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[3,1] <- 'reshaped_ensemble'
model_summary[3,2] <- cm3$overall[1]
model_summary[3,3] <- find_minor_f(cm3)

model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[4,1] <-'base_multinom'
model_summary[4,2] <- cm_mn$overall[1]
model_summary[4,3] <- find_minor_f(cm_mn)

cm_mn_r <- confusionMatrix(new_test_results$mn, testing$quality)
model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[5,1] <- 'reshaped_multinom'
model_summary[5,2] <- cm_mn_r$overall[1]
model_summary[5,3] <- find_minor_f(cm_mn_r)

model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[6,1] <-'base_naive_bayes'
model_summary[6,2] <- cm_nb$overall[1]
model_summary[6,3] <- find_minor_f(cm_nb)

cm_nb_r <- confusionMatrix(new_test_results$nb, testing$quality)
model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[7,1] <- 'reshaped_naive_bayes'
model_summary[7,2] <- cm_nb_r$overall[1]
model_summary[7,3] <- find_minor_f(cm_nb_r)

model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[8,1] <-'base_random_forest'
model_summary[8,2] <- cm_rf$overall[1]
model_summary[8,3] <- find_minor_f(cm_rf)

cm_rf_r <- confusionMatrix(new_test_results$rf, testing$quality)
model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[9,1] <- 'reshaped_random_forest'
model_summary[9,2] <- cm_rf_r$overall[1]
model_summary[9,3] <- find_minor_f(cm_rf_r)

model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[10,1] <-'base_boosting'
model_summary[10,2] <- cm_gbf$overall[1]
model_summary[10,3] <- find_minor_f(cm_gbf)

cm_gbf_r <- confusionMatrix(new_test_results$gbm, testing$quality)
model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[11,1] <- 'reshaped_boosting'
model_summary[11,2] <- cm_gbf_r$overall[1]
model_summary[11,3] <- find_minor_f(cm_gbf_r)

model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[12,1] <-'base_linear_discriminant'
model_summary[12,2] <- cm_lda$overall[1]
model_summary[12,3] <- find_minor_f(cm_lda)

cm_lda_r <- confusionMatrix(new_test_results$lda, testing$quality)
model_summary <- rbind(model_summary, rep(NA, 3))
model_summary[13,1] <- 'reshaped_linear_discrim'
model_summary[13,2] <- cm_lda_r$overall[1]
model_summary[13,3] <- find_minor_f(cm_lda_r)

print(model_summary, row.names=FALSE)
```

```{r result barplot, echo=FALSE}
# need barplot code here


```

With a model based on our reshaped data, we are seeing a decrease in overall
accuracy for both our ensemble model, and each of the individual components
of that model.  LDA and Naive Bayes both show rather drastic drop-offs in
accuracy, perhaps reinforcing our earlier observation that these models should
have been dropped after our initial analysis.

On the flip side, we're seeing that, other than with the Naive Bayes model, the
minority class f-score average improves with the resampled data.

This is pretty much what we'd expect to see, as the undersampling causes some
accuracy loss with the majority classes.  Similarly, minority class performance
improves as oversampling provides increased information which our model can 
use to identify characteristics of these minority classes.

## Comments on Naive Bayes Weakness

One of the more surprising results of this work (at least to me) was the 
severe degradation of the Naive Bayes Model's accuracy as a result of our class
rebalancing.  

On reflection, however, this isn't all that shocking.  Naive Bayes' main 
strengths as a classifier are that it is fairly light-weight, with good
accuracy, and that it doesn't need tons of data to work well.^[See, for example
Forman and Cohen, p12, who note that "Naive Bayes models are often relatively 
insensitive to a shift in training distribution"]  This compares rather starkly
with the Random Forest approach which is quite resource-intensive to build, and
that readily improves with additional data points.

In other words, class imbalance seems to be less of a problem with the 
probabilistic methodologies that underpin both Naive Bayes and LDA.  As such,
if a light-weight, average performance model is acceptable, Naive Bayes is 
probably a good choice, especially in that class rebalancing does not appear to 
be particularly necessary, or even helpful.

# Comparison of Results to Original Paper and Conclusions

## Differences in Approach

As noted above, there are three main differences between our approach and that
taken in the original paper:

1. We chose to apply a classification, rather than regression approach

2. We relied on an ensemble learning approach, rather than a single model.

3. We transformed the original data through over/under sampling.

The comparative results are shown just above in the results summary table.

At the end of the day, however, our ensemble approach achieved pretty similar
results to those in the original papers, albeit with a whole lot more work.

## Ensemble model vs. SVM (Original model)

The ensemble approach we used actually returned slightly weaker accuracy than 
the original paper's SVM approach.  In addition, the average f-statistic for
minority classes was weaker in our model, than for the SVM approach in the
original paper.

Part of the reason for this weaker performance was the relative weakness of 
the probabilistic approaches of Naive Bayes and LDA, and the relative overweight
that these models had in contributing to the final evaluation.

A better practice to building the ensemble model would be to first evaluate
the predictive effectiveness of a number of different approaches, and then
combine the most effective approaches into a final model.

## Impact of class rebalancing

The impact of the over/under sampling algorithm we introduced was interesting.
It reduced the overall accuracy of the ensemble model by about 10%, while 
boosting the average f-stat for minority classes by about 20%.  Overall 
predictive ability for minority classes was still rather weak even after this
transformation.

With our random forest model, on the other hand, the gains in minority class
predictive quality were a bit more pronounced, with our f-stat rising to 0.18,
with a very small drop-off in overall predictive ability.  Still, though, this
is not a particularly impressive result when we look at how well we did at 
predicting good or bad wines.

Overall, it appears, from this one example, that if minority class prediction
is an important metric for model evaluation, then class rebalancing may be 
worthwhile.  If the main criterion is overall accuracy, then rebalancing may 
not be worth the effort, as it will almost certainly reduce accuracy.

## Final comments

Finally, it's worth noting that this entire exercise in predicting quality based
on chemical ratings may be flawed to begin with.  Price, for example, has been
shown to have a significant impact on impression of wine quality - an impact
that can be verified through brain MRI scans.^[Plassman et al, "Marketing 
Actions Can Modulate Neural Representations of Experienced Pleasantness"]  In
other words, if we know that people's *physical* experience of pleasure from
a wine can be influenced by the wine's price point, it's hard to believe that
chemical analysis is really going to be able to provide a strongly-reliable
set of predictors of wine quality.

This malleable aspect of quality ratings probably puts an upper bound on the 
ability of any model to predict quality based on chemical ratings.  In the 
original paper, the white wine predictions were not noticeably better than the
red wine predictions (SVM gave 64.6% accuracy at T=0.5 for white, vs 62.4% for
red).  This is in spite of the fact that the data set is three times the size of 
the red.

The random forest model is clearly head and shoulders above the other options 
for its combination of accuracy and minority class predictive ability.  But it
is worth investigating whether a better composed ensemble learning model could
improve on this sole model.

\newpage

# Appendix 1 - Model code

This function has been coded **only** to run and evaluate the classification 
algorithm.  It doesn't actually return the models that are developed - it only 
returns the predictions for the final model and each of the intermediate models. 
If the actual models are needed, it's a fairly trivial change to actually return 
them.

```{r appendix1}
# Helper function to get mode of a vector
getMode

# Main code, including test set evaluation algorithm
ensemble_eval
```

\newpage

# Appendix 2 - Over/Under Sample and Multiclass SMOTE Algorithm code

Pseudocode for the wrapper, undersample & undersample algorithms can be found
in the Chawla et al "Automatically Countering Imbalance".

Pseudocode for the SMOTE algorithm can be found in Chawla et al "SMOTE Synthetic 
Minority Over-sampling"

```{r appendix2}
wrapper

wrap_undersample

undersample

wrap_smote

smote_sample

smote

populate

# Custom evaluation function
evaluate_model
```

\newpage

# Bibliography

Chawla NV, Cieslak DA, Bowyer KW, Kegelmeyer WP SMOTE Synthetic Minority 
Over-sampling TEchnique Journal of Artificial Intelligence Research. 
2002;16:341-78

Chawla NV, Cieslak DA, Hall LO, Joshi A. Automatically countering imbalance and 
its empirical relationship to cost. Data Mining and Knowledge Discovery.
2008;17(2):225-52.

Cortez P, Cerdeira A, Almeida F, Matos T, Reis J. Modeling wine preferences by 
data mining from physicochemical properties. Decision Support Systems. 
2009;47(4):547-53.

Foreman, George and Cohen, Ira, Learning from Little: Comparison of Classifiers
Given Little Training, Hewlett-Packard Research Laboratories, 15th European 
Conference on Machine Learning and the 8th European Conference on Principles and 
Practice of Knowledge Discovery in Databases, 20-24 September 2004, Pisa, Italy 
http://www.ifp.illinois.edu/~iracohen/publications/precision-ecml04-ColorTR-final.pdf

Plassmann H, O'Doherty J, Shiv B, Rangel A. Marketing Actions Can Modulate 
Neural Representations of Experienced Pleasantness. Proceedings of the National
Academy of Sciences of the United States of America. 2008;105(3):1050-4.

Qian Y et al., A resampling ensemble algorithm for classification of imbalance 
problems, Neurocomputing (2014), http://dx.doi.org/10.1016/j.neucom.2014.06.021i

Rokach L. Ensemble-based classifiers. Artificial Intelligence Review. 
2010;33(1):1-39.
