---
title: 'Assignment 3: CMTH642'
author: "Christopher Graham"
date: "November 27, 2015"
output: pdf_document
---

# Overview **NEED TO REWRITE**

This assignment develops a model to predict a subjective quality rating on 
Portuguese red vinho verdes.  The data is provided at
https://archive.ics.uci.edu/ml/datasets/Wine+Quality and was originally used for
the paper "Modeling wine preferences by data mining from physicochemical
properties" available at
http://repositorium.sdum.uminho.pt/bitstream/1822/10029/1/wine5.p df

The data set provides provides 11 chemical measurements of Portuges red wines,
and one quality score assigned by human testers.  The data is complete and
clean.  Pre-processing is described in the original paper.

Our goal is to develop a model that will predict the quality rating of a wine
based on these chemical measurements.  Numbered sections of this paper 
correspond to the numbered instructions in the assignment sheet.

# 1. Import and Split Data

```{r load, message = FALSE}
# load all libraries required in analysis
require(caret)
require(corrplot)

red <- read.csv('winequality-red.csv', sep = ';')
# Treating as classification problem so convert quality to factor variable
red$quality <- as.factor(red$quality)

# Divide off a testing set for final validation
# All work will be done only based on the training set
set.seed(25678)
train_idx <- createDataPartition(y=red$quality, p=0.8, list = FALSE)
training <- red[train_idx,]
testing <- red[-train_idx,]
```

# 2. Check Data Characteristics

## Completeness

```{r check data}
if (sum(complete.cases(testing)) == nrow(testing)) {
    print('All complete cases')
}
```

The data is complete in all variables, and as such there is no need to impute
values.  

## Class Balance

But the data isn't perfect.  The biggest problem is with the 
distribution of quality scores.  Specifically, over 80% of the wines have 
average ratings (5 or 6), and very few wines get ratings at the more extreme
ends of the rating spectrum.  (3 and 8 are the extreme values awarded, even
though wines were rated on a 10-point scale).  Class imbalance has been
shown to be a significant problem in building an effective Machine Learning
model.  We  provide a strategy for dealing with this below, but first will
look at some other characteristics of the data.

```{r qual histogram, echo=FALSE}
ggplot(testing, aes(x=quality)) +
    geom_histogram(color='black', fill='white') +
    ggtitle('Distribution of Quality Scores') +
    xlab('Quality Score')
```

## Variable Relationships

```{r}
# Check for variables with near zero variability
nearZeroVar(training, saveMetrics = TRUE)

# look at correlation between predictor variables
corrplot(cor(training[-12]), method = 'ellipse', type = 'lower',
         title = 'Correlation of predictor variables', diag=FALSE,
         mar=c(0,0,1,0))

# Compare relationsip of quality to predictor variables
# first normalize the variables so we can compare in one graph
norm_obj <- preProcess(training[,-12], method=c('center', 'scale'))
train_norm <- predict(norm_obj, training)
featurePlot(x=train_norm[,-12], y=train_norm$quality, plot='box',
            main = 'Relation of quality to predictor variables')
```

What this preliminary exploration tells us is:

- None of the variables show near zero variability, meaning there is no 
*prima facie* reason to exclude any at the start of our analysis
- For the most part, there does not seem to be a huge amount of collinearity
between variables.  There are some instances of collinearity, where it might be
expected (citric.acid, fixed.acidity and pH), (free.sulfur.dioxide and
total.sulfur.dioxide).  So there may be some room to reduce dimensionality.
- There are some clear relationships between some of the predictor variables 
and wine quality.  volatile.acidity, alcohol and density seem particularly
important.

## Strategies for dealing with Class Imbalance in Provided Data

Since one of the goals for a prediction system would be to help vintners 
identify wines that are likely to be either very good or very bad, the lack
of wine specimens with extreme scores could be problematic for our model
development.

Dealing with class imbalance is a common problem in machine learning.  The 
standard approach in this situation is a combination of under-sampling the 
majority class and over-sampling the minority class to create a distribution
that will help to develop an adequate predictive model.  However, there is a 
persistent concern about the extent to which this under & oversampling should 
occur, and the specific techniques to implement.

While there are different ways of approaching this problem, many of them 
are designed around a binary response variable.  Specifically, the R package 
'unbalanced' provides a number of pre-made approaches to dealing with an 
unbalanced data set.  But, unfortunately, it requires a binary response 
variable.

In this case, we are approaching this as a multiple-level classification, 
relying on the classes provided in the original data (reasons for this are 
discussed below). 

I have identified two different approaches to effectively determining an over-/
under-sampling approach for multi-class response variables.  The first is the 
wrapper approach proposed by Chawla et al^[Chawla NV, Cieslak DA, Hall LO, 
Joshi A. Automatically countering imbalance and its empirical relationship to
cost. *Data Mining and Knowledge Discovery*. 2008;17(2):225-52.], and the 
resampling ensemble algorithm proposed by Qian et al^[Y. Qian, et al., A
resampling ensemble algorithm for classification of imbalance problems,
*Neurocomputing* (2014), http://dx.doi.org/10.1016/j.neucom.2014.06.021i].

Both of these approaches rely on a combination of SMOTE oversampling and random
undersampling.  However, the Chawla paper offers a more coherent (and KDD-Cup
proven!) approach to determining sample levels, and so we will use that
approach here.

Note, however that the algorithms used in Chawla et al require us to identify
a classifier algorithm that will be used to determine stop points for sampling.

So, we need to turn briefly to model selection.

# 3. Model Selection

## Regression vs. Classification

As noted on the source page for this data set, the data lends itself to either
a regression or classification approach.  The original paper to use this data
set adopted a regression appraoch to the data, arguing that regression allows us
to better evaluate "near miss" predictions (e.g. if the true value is 3, we can
up-score the model if it predicts 4, rather than 7).  The paper used both Neural
Network (NN) and Support Vector Machine (SVM) models, and obtained an accuracy
rate of 64% with a 0.5 error tolerance in quality prediction, and 89% accuracy 
with a 1.0 error tolerance.^[Cortez P, Cerdeira A, Almeida F, Matos T, Reis J.
Modeling wine preferences by data mining from physicochemical properties. 
Decision Support Systems. 2009;47(4):547-53.]

Furthermore, given the somewhat arbitrary way in which quality was scored 
(median value from 3 or more judges), it's safe to say that these are not 
absolute values, but rather approximations.  As such, regression probably
makes most sense.

But Assignment 1 was based around regression, and I'd like to make this one a
bit more challenging, so I'm going to frame it as a classification problem.
This means the fairest comparison for our results is with the 0.5 tolerance 
level in the original paper as a difference of less than 0.5 would round to 
the correct value.

As a classification problem, our primary evaluation metric will be 
classification accuracy.

## Performance Criteria

In order to ensure easy comparability with the original paper, and for 
simplicity of understanding, our primary judge of performance will be overall
model accuracy.

However, overall accuracy does not indicate how effective our model is at 
identifying minority class examples.

However, following the arguments made by Chawla et al, our over/under sampling
algorithm will be based on the model's f-statistic:

\[
    f measure = \frac{2 * precision * recall}{recall + precision}
\]

**NEED TO WRITE A BIT MORE HERE**

## Model Selection

Given the relatively weak predictive accuracy (at T=0.5) for the models in the
first paper, it appears that a single model may not be all that great at
classification.  In these instances, one of the more interesting approaches
is *ensemble learning* - essentially combining predictions from multiple models
into one super-model.^[Numerous articles have shown this, but see specifically:
Rokach L. Ensemble-based classifiers. Artificial Intelligence Review.
2010;33(1):1-39.]

In terms of specific ensemble learning implementation, we are going to use the
approach suggested by Jeff Leek of John's Hopkins in the Coursera course 
"Practial Machine Learning"^[Course notes available at: http://sux13.github.io/DataScienceSpCourseNotes/8_PREDMACHLEARN/Practical_Machine_Learning_Course_Notes.pdf, starting on p. 67].  This is essentially a stacking
algorithm.

In order to build an ensemble model, we first need to build a number of
individual models to combine into the final approach.  In an attempt to bring
together all we've done in the course (plus a little bit more!), we're going
to use:

- multinomial logistic regression (multinom or mn)
- NaÃ¯ve Bayes (nb)
- Random Forest (rf)
- Boosting; specifically boosting with trees (gbm)
- Linear Discriminant Analysis (lda) - to see if there are any interesting
differences with Naive Bayes if we don't assume variable independence

We are specifically excluding the two models of the original paper to see if
we can do better without the models used there.

Also, that because the SMOTE oversampling algorithm relies on a knn methodology,
we will not be including knn in the base models in order to avoid potentially
introducing erroneous re-classification.

Finally, note that for the final model, the model we will use is a simple
plurality vote among the model responses (the modal value).

# Implementation of Ensemble Model

## Normalizing Data

As per the original paper, we will be evaluating the feed-in models based on
normalized data.  This is done through the trainControl parameter in caret.

## Model creation and baseline testing

Since this is a fairly complicated model that needs to be incorporated into
the rebalancing algorithm, we're going to create a function to return the final
model for evaluation.

Note that our model evaluation is set using 10 repetitions of 10-fold 
cross-validation, and that the input data is normalized on each iteration of the
model.

```{r model code, message=FALSE}
# Function to use in final
getMode <- function(vect) {
    unq_vect <- unique(vect)
    return(unq_vect[which.max(tabulate(match(vect, unq_vect)))])
}


ensemble_eval <- function(train_set) {
    # Input - training set
    # Output - list of final predictions, intermediate models & normalizer
    # note - model is hard-coded to use quality as output variable
    
    # Set parameters for modedl build
    fit_control <- trainControl(## 10-fold CV
                                method = "repeatedcv",
                                number = 10, 
                                ## repeated ten times
                                repeats = 10)

    # normalize data with returnable object
    norm_obj <- preProcess(train_set[,-12], method=c('center', 'scale'))
    train_set <- predict(norm_obj, train_set)

    set.seed(15954)
    mn_model <- train(quality ~ .,
                      data = train_set,
                      method = 'multinom',
                      trControl = fit_control,
                      trace = FALSE)
    mn_pred <- predict(mn_model, train_set)

    set.seed(15954)    
    nb_model <- train(quality ~ .,
                      data = train_set,
                      method = 'nb',
                      trControl = fit_control)
    nb_pred <- predict(nb_model, train_set)

    set.seed(15954)
    rf_model <- train(quality ~ .,
                      data = train_set,
                      method = 'rf',
                      trControl = fit_control)
    rf_pred <- predict(rf_model, train_set)

    set.seed(15954)
    gbm_model <- train(quality ~ .,
                       data = train_set,
                       method = 'gbm',
                       trControl = fit_control,
                       verbose=FALSE)
    gbm_pred <- predict(gbm_model, train_set)

    set.seed(15954)
    lda_model <- train(quality ~ .,
                       data = train_set,
                       method = 'lda',
                       trControl = fit_control)
    lda_pred <- predict(lda_model, train_set)
   
    ensemble_data <- data.frame(mn = mn_pred,
                                nb = nb_pred,
                                rf = rf_pred,
                                gbm = gbm_pred,
                                lda = lda_pred)

    final_pred <- apply(ensemble_data, 1, function(x) getMode(x))

    return(list(final_model = final_pred,
                mn_model = mn_model,
                nb_model = nb_model,
                rf_model = rf_model,
                gbm_model = gbm_model,
                lda_model = lda_model,
                norm_obj = norm_obj))
}

```

As a baseline, before dealing with the class imbalance, let's see how our
model performs:

```{r inital results, cache=TRUE, message=FALSE, warning=FALSE}
# First, get the final and interim model results
base_model <- ensemble_eval(training)
```

```{r evaluate base model}
model_summary <- data.frame(model=names(base_model)[1:6], accuracy=rep(0,6))
model_summary[1,2] <- confusionMatrix(base_model[[1]], 
                                      training$quality)$overall[1]
for (i in 2:6) {
    model_summary[i,2] <- base_model[[i]]$results$Accuracy[1]
}
model_summary
```

```{r cm base combo}
confusionMatrix(base_model[[1]], training$quality)
```

## Baseline against test set

Thm model gives us a pretty good result overall, with a predicted 71% accuracy 
on the training data, beating than the 64% accuracy (at a 0.5 threshhold) from 
the original paper.  As a test, we'll apply this model to the test data and see 
if this is legitimate or due to overfitting.

```{r test eval function}
evaluate_test <- function(model_list, test_set) {
    # 1. apply normalization model
    test_set <- predict(model_list[[7]], test_set)
    # 2-3 run base models and create interim result data frame
    ensemble_data <- data.frame(matrix(NA, nrow=nrow(test_set), ncol=0))
    for (i in 2:6) {
        print(i)
        new_pred <- predict(model_list[[i]], test_set)
        ensemble_data <- cbind(ensemble_data, new_pred)
    }
    colnames(ensemble_data) <- c('mn', 'nb', 'rf', 'gbm', 'lda')
    # 4. apply modal selection to interim data frame
    test_pred <- apply(ensemble_data, 1, function(x) getMode(x))
    ensemble_data <- cbind(final = test_pred, ensemble_data)
    return(ensemble_data)
}
```

```{r evaluate test}
test_pred <- evaluate_test(base_model, testing)
class_sensitivity <- data.frame(matrix(NA, nrow=0, ncol=6))
model_summary <- cbind(model_summary, test_results = rep(0,6))
for (i in 1:ncol(test_pred)) {
    cm <- confusionMatrix(factor(test_pred[,i], levels = c(3,4,5,6,7,8)),
                          testing$quality)
    model_summary[i,3] <- cm$overall[1]
    class_sensitivity <- rbind(class_sensitivity, cm$byClass[,1])
}
colnames(class_sensitivity) <- row.names(cm$byClass)
class_sensitivity <- cbind(model = colnames(test_pred), class_sensitivity)
model_summary
class_sensitivity
```

Interestingly, what we see here is that, for the test set, our final model
has a significantly degreaded peformance, even though each of the individual
models actually had improved performance.

And when we drill down, what we're seeing is that while the model is doing a
decent job predicting 5 & 6 values, our specificity for the rare classes 
(3, 4, 8, and to a lesser extent, 7) is quite appalling.

So, in an effort to improve on this, we need to turn to our function for 
undersampling and SMOTE oversampling.

# Undersampling/SMOTE oversampling implementation

As discussed earlier, I have decided to implement the "Wrapper Undersample
SMOTE Algorithm" developed by Chawla et al as a way to rebalance the provided
data.

Simplified, the algorithm works as follows:

1) Establish a baseline performance level for each majority and minority class
2) For each Majority class, undersample by 10% until either minority class
predictions stop improving, or until majority class predections degrade by 5%.
3) For each Minority class, SMOTE oversample by 100%, and continue this 
until performance is increased by less than 5%.

Perfomance is evaluated on f-stat, which is:

**PUT F-STAT HERE IN MATH FORMAT**

Specific

```{r wrapper algo}
source('over_under_wrapper.R')
```



```{r engine='python'}
print('this is python 2.7.10')
import sys
print(sys.version)

```


3. (30 points) Propose a model for the prediction. Give a few reasons for
your selection briefly. You may choose to model the problem as classification or regression. Define the task, experience and performance criteria.
4. (30 points) Do 10 fold cross validation in the experiment.
5. (20 points) Report your results.
6. (Bonus 20 points) Other researchers have built predictive models using
this data. Do a brief comparison of your results with the result in the following paper: http://repositorium.sdum.uminho.pt/bitstream/1822/10029/1/wine5.pdf
